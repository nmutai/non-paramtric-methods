inits    = list(mu=Ybar),
n.chains = 1)
update(model1, 1000)
samp <- coda.samples(model1, variable.names=c("sigma","pct","gamma"), n.iter=2000)
plot(samp)
summary(samp)
#logistic regression with random effects on NCAAFB data
library(rjags)
dat <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/CFB2014.csv", stringsAsFactors = FALSE)
# The winning and losing teams are in columns 1 and 2
w   <- dat[,1]
l   <- dat[,2]
# Covert team names to team ID numbers
teams  <- sort(unique(c(w,l)))
n      <- length(w)
m      <- length(teams)
ID     <- matrix(NA,n,2)
record <- matrix(NA,m,2)
for(i in 1:m){
ID[,1] <- ifelse(w==teams[i],i,ID[,1])
ID[,2] <- ifelse(l==teams[i],i,ID[,2])
record[i,1] <- sum(w==teams[i])
record[i,2] <- sum(l==teams[i])
}
colnames(record) <- c("W","L")
rownames(record) <- teams
record[1:20,]
ID[,1] <- ifelse(w==teams[i],i,ID[,1])
library(rjags)
dat <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/CFB2014.csv", stringsAsFactors = FALSE)
# The winning and losing teams are in columns 1 and 2
w   <- dat[,1]
l   <- dat[,2]
# Covert team names to team ID numbers
teams  <- sort(unique(c(w,l)))
n      <- length(w)
m      <- length(teams)
ID     <- matrix(NA,n,2)
record <- matrix(NA,m,2)
for(i in 1:m){
ID[,1] <- ifelse(w==teams[i],i,ID[,1])
ID[,2] <- ifelse(l==teams[i],i,ID[,2])
record[i,1] <- sum(w==teams[i])
record[i,2] <- sum(l==teams[i])
}
colnames(record) <- c("W","L")
rownames(record) <- teams
record[1:20,]
CFB_model <- "model{
# Likelihood
for(i in 1:n){
one[i] ~ dbern(p[i])
logit(p[i]) <- RE[ID[i,1]] - RE[ID[i,2]]
}
# Priors
for(j in 1:m){
RE[j] ~ dnorm(0,inv.var)
}
inv.var ~ dgamma(0.1,0.1)
}"
#compile the model in JAGS
update(model1, 10000)
samp <- coda.samples(model1,variable.names=c("RE"),n.iter=20000)
#create the model in JAGS
CFB_model <- "model{
# Likelihood
for(i in 1:n){
one[i] ~ dbern(p[i])
logit(p[i]) <- RE[ID[i,1]] - RE[ID[i,2]]
}
# Priors
for(j in 1:m){
RE[j] ~ dnorm(0,inv.var)
}
inv.var ~ dgamma(0.1,0.1)
}"
one <- rep(1,n) #for the winners
model1 <- jags.model(file     = textConnection(CFB_model),
data     = list(one=one,ID=ID,n=n,m=m),
inits    = list(RE=rep(0,m)),
n.chains = 1
)
#compile the model in JAGS
update(model1, 10000)
samp <- coda.samples(model1,variable.names=c("RE"),n.iter=20000)
#convergance diagnostics
RE <- samp[[1]]
#plot some of the random effects just to see
plot(RE[,1:3])
effectiveSize(RE[,these])
RE <- as.matrix(RE)
colnames(RE) <- teams
# Convert random effects to ranks
rank <- RE
for(i in 1:nrow(RE)){
rank[i,] <- rank(-RE[i,])
}
#Sort teams by median rank
med_rank <- apply(rank,2,median)
RE <- RE[,order(med_rank)]
rank <- rank[,order(med_rank)]
teams <- teams[order(med_rank)]
# Create a table to summarize the posterior rank of the top 25 teams
median <- apply(rank,2,median)
q05 <- apply(rank,2,quantile,0.05)
q95 <- apply(rank,2,quantile,0.95)
prob1  <- apply(rank==1,2,mean)
output <- cbind(median,q05,q95,prob1)
output[1:25,]
# Compare to true rankings
rank0 <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/espnPowerRankings.csv")
rank1 <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/playoffRankings.csv")
cbind(rank0,rank1,output[1:25,])
# Plot the posterior of the ranks for the top 25 teams
boxplot(rank[,1:25],las=3,cex.axis=.75,ylab="Posterior rank")
# Compute the posterior head-to-head win probabilities
# Prob(team i beats team j)
expit <- function(x){exp(x)/(1+exp(x))}
pct <- matrix(NA,10,10)
colnames(pct) <- rownames(pct) <- teams[1:10]
for(i in 1:10){for(j in 1:10){if(i!=j){
pct[i,j] <- mean(expit(RE[,i]-RE[,j]))
}}}
round(pct,2)
#Summary: How'd we do? Ohio State beat Florida State in the National Championship game.
#The posterior probability that Ohio State was the best team in the county was only about 0.03,
#and the probability that they would beat Florida State was only 0.3. So our predictions leave
#a lot to be desired!
#logistic regression with random effects on NCAAFB data
library(rjags)
dat <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/CFB2014.csv", stringsAsFactors = FALSE)
# The winning and losing teams are in columns 1 and 2
w   <- dat[,1]
l   <- dat[,2]
# Covert team names to team ID numbers
teams  <- sort(unique(c(w,l)))
n      <- length(w)
m      <- length(teams)
ID     <- matrix(NA,n,2)
record <- matrix(NA,m,2)
for(i in 1:m){
ID[,1] <- ifelse(w==teams[i],i,ID[,1])
ID[,2] <- ifelse(l==teams[i],i,ID[,2])
record[i,1] <- sum(w==teams[i])
record[i,2] <- sum(l==teams[i])
}
colnames(record) <- c("W","L")
rownames(record) <- teams
#create the model in JAGS
CFB_model <- "model{
# Likelihood
for(i in 1:n){
one[i] ~ dbern(p[i])
logit(p[i]) <- RE[ID[i,1]] - RE[ID[i,2]]
}
# Priors
for(j in 1:m){
RE[j] ~ dnorm(0,inv.var)
}
inv.var ~ dgamma(0.1,0.1)
}"
one <- rep(1,n) #for the winners
model1 <- jags.model(file     = textConnection(CFB_model),
data     = list(one=one,ID=ID,n=n,m=m),
inits    = list(RE=rep(0,m)),
n.chains = 1
)
#compile the model in JAGS
update(model1, 10000)
samp <- coda.samples(model1,variable.names=c("RE"),n.iter=20000)
#convergance diagnostics
RE <- samp[[1]]
#plot some of the random effects just to see
plot(RE[,1:3])
effectiveSize(RE[,these])
RE <- as.matrix(RE)
colnames(RE) <- teams
# Convert random effects to ranks
rank <- RE
for(i in 1:nrow(RE)){
rank[i,] <- rank(-RE[i,])
}
#Sort teams by median rank
med_rank <- apply(rank,2,median)
RE <- RE[,order(med_rank)]
rank <- rank[,order(med_rank)]
teams <- teams[order(med_rank)]
# Create a table to summarize the posterior rank of the top 25 teams
median <- apply(rank,2,median)
q05 <- apply(rank,2,quantile,0.05)
q95 <- apply(rank,2,quantile,0.95)
prob1  <- apply(rank==1,2,mean)
output <- cbind(median,q05,q95,prob1)
output[1:25,]
# Compare to true rankings
rank0 <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/espnPowerRankings.csv")
rank1 <- read.csv("http://math.luc.edu/~ebalderama/stat388bayes_resources/data/playoffRankings.csv")
cbind(rank0,rank1,output[1:25,])
# Plot the posterior of the ranks for the top 25 teams
boxplot(rank[,1:25],las=3,cex.axis=.75,ylab="Posterior rank")
# Compute the posterior head-to-head win probabilities
# Prob(team i beats team j)
expit <- function(x){exp(x)/(1+exp(x))}
pct <- matrix(NA,10,10)
colnames(pct) <- rownames(pct) <- teams[1:10]
for(i in 1:10){for(j in 1:10){if(i!=j){
pct[i,j] <- mean(expit(RE[,i]-RE[,j]))
}}}
round(pct,2)
#Summary: How'd we do? Ohio State beat Florida State in the National Championship game.
#The posterior probability that Ohio State was the best team in the county was only about 0.03,
#and the probability that they would beat Florida State was only 0.3. So our predictions leave
#a lot to be desired!
library(dplyr)
getwd()
train = read.csv("hw5/data/smogData_train.csv")
train = read.csv("hw5/data/smogData_train.csv")
str(train)
plot(train)
cor(train[,1:])
cor(train[,1:10])
library(corrplot)
corrplot(train[,1:10], method = "number")
library(corrplot)
install.packages(corrplot)
install.packages(corrplot)
install.packages("corrplot")
library(corrplot)
corrplot(train[,1:10], method = "number")
corrplot(train[,1:10], method = "number")
cor(train[,1:10])
corrplot(train, method = "number")
train_cors = cor(train[,1:10])
corrplot(train_cors, method = "number")
train_cors = cor(train[,1:10], method = 'spearman')
corrplot(train_cors, method = "number")
train_cors = cor(train[,1:10], method = 'tau')
train_cors = cor(train[,1:10], method = 'kendall')
corrplot(train_cors, method = "number")
library(tree)
install.packages('tree')
library(tree)
basic_tree = tree(03~.,data=train)
basic_tree = tree(o3 ~ .,data=train)
basic_tree = tree(o3 ~ ., train)
train$O3
basic_tree = tree(train$o3 ~ ., train)
basic_tree = tree(o3 ~ temp + vh + ibh + ibt ., train)
basic_tree = tree(o3 ~ temp + vh + ibh + ibt, train)
basic_tree = tree(o3 ~ temp + vh + ibh + ibt, data=train)
Y < train$o3
Y = train$o3
basic_tree = tree(Y ~ temp + vh + ibh + ibt, data=train)
basic_tree = tree(o3 ~ temp + vh + ibh + ibt, data=train)
basic_tree = tree(o3 ~ temp + vh + ibh + ibt, data=train)
basic_tree = tree(o3~temp+vh+ibh+ibt, data=train)
basic_tree = tree(o3~temp+vh+ibh+ibt, data=train)
rename(train, 'o3 = ozone')
basic_tree = tree(o3~temp+vh+ibh+ibt, data=train)
basic_tree = tree(o3~temp+vh+ibh+ibt, data=train)
basic_tree = tree(temp~vh+ibh+ibt, data=train)
basic_tree = tree(o3~temp+vh+ibh+ibt, data=train)
names(train)
train$X
names(train)[2]
names(train)[2] = Ozone
names(train)[2] = 'Ozone'
names(train)
train = read.csv("hw5/data/smogData_train.csv")
#rename o3 so the model doesnt get pissy
names(train)[2] = 'Ozone'
basic_tree = tree(ozone~temp+vh+ibh+ibt, data=train)
train$Ozone
names(train)[2] = 'ozone'
basic_tree = tree(ozone~temp+vh+ibh+ibt, data=train)
plot(basic_tree)
label(basic_tree)
plot(basic_tree)
label(basic_tree)
text(basic_tree)
full_tree = tree(ozone~.,data=train)
plot(full_tree)
text(full_tree)
plot(basic_tree)
summary(full_tree)
test = read.csv("HW5/DATA/smogData_test.csv")
names(test)[2] = 'ozone'
predict(full_model, test)
predict(full_tree, test)
summry(predict(full_tree, test))
summary(predict(full_tree, test))
summary(predict(full_tree, test))
summary(full_tree)
partition.tree(full_tree)
predict(ffull_tree, test)
predict(full_tree, test)
mse(predict(full_tree, test))
mse = mean((predict(full_tree, test) - test$ozone)^2)
mse
library(randomForest)
randomForest(oxone~., train,ntree=100)
randomForest(ozone~., train,ntree=100)
rf = randomForest(ozone~., train,ntree=100)
plot(rf)
text(rf)
summary(rf)
rf
rf_mse = mean((predict(fr, test) - test$ozone)^2)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf_mse
rf = randomForest(ozone~., train,ntree=1000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf_mse
rf = randomForest(ozone~., train,ntree=10000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf_mse
rf = randomForest(ozone~., train,ntree=100000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
preds = c(train$vh,train$wind,train$humidity,train$temp,train$ibh,train$dpg,train$ibt,train$vis,train$doy)
rf_cross = rfcv(train$ozone, preds, cv.fold = 10)
train[,2,10]
train[,2:10]
train[,3:11]
View(train_cors)
View(train_cors)
rf_cross = rfcv(train[,3:11], train$ozone, cv.fold = 10)
rf_cross
with(rf_cross, plot(n.var, error.cv, log="x", type="o", lwd=2))
cross_mse = mean((predict(rf_cross, test) - test$ozone)^2)
rf_cross[1]
rf_cross[2]
importance(rf_cross)
importance(rf)
plot(importance(rf))
importance(rf)
rf = randomForest(ozone~., train,ntree=1000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf_mse
rf = randomForest(ozone~., train,ntree=10000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf_mse
rf1 <- randomForest(ozone~., train,ntree=1000, norm.votes=FALSE)
rf2 <- randomForest(ozone~., train,ntree=1000, norm.votes=FALSE)
rf3 <- randomForest(ozone~., train,ntree=1000, norm.votes=FALSE)
rf.all <- combine(rf1, rf2, rf3)
print(rf.all)
importance(rf.all)
rf_mse = mean((predict(rf.all, test) - test$ozone)^2)
rf.all_mse = mean((predict(rf.all, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=1000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
importance(rf)
rf.all_mse = mean((predict(rf.all, test) - test$ozone)^2)
rf.all_mse
rf3 <- randomForest(ozone~., train,ntree=10000, norm.votes=FALSE)
rf1 <- randomForest(ozone~., train,ntree=10000, norm.votes=FALSE)
rf2 <- randomForest(ozone~., train,ntree=10000, norm.votes=FALSE)
rf3 <- randomForest(ozone~., train,ntree=10000, norm.votes=FALSE)
rf.all <- combine(rf1, rf2, rf3)
rf.all_mse = mean((predict(rf.all, test) - test$ozone)^2)
rf.all_mse
rf = randomForest(ozone~., train,ntree=1000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
plot(rf)
rf = randomForest(ozone~., train,ntree=100)
plot(rf)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=500)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=750)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=1000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=10000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=10000)
rf = randomForest(ozone~., train,ntree=100000)
plot(rf)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
rf = randomForest(ozone~., train,ntree=1000000)
rf_mse = mean((predict(rf, test) - test$ozone)^2)
help("randomForest")
rf_opt = randomForest(ozone~., train,ntree=1000000, nodesize = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 3)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 10)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 5)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
MSE_rand = rep(NA,1:30)
MSE_rand = rep(NA,30)
for( i in 1:30){
temp_opt = randomForest(ozone~., train,ntree=1000, nodesize = i)
MSE_rand[i] = mean((predict(temp_opt, test) - test$ozone)^2)
}
min(MSE_rand)
MSE_rand
plot(MSE_rand)
min(MSE_rand)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
help("randomForest")
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 7)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 6)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 5)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 3)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 1)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 1)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=100000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=10000, nodesize = 1, mtry = 2)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=10000, nodesize = 1, mtry = .3)
plot(rf_opt)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
MSE_rand = rep(NA,11)
for (i in 1:11){
temp_opt = randomForest(ozone~., train,ntree=10000, nodesize = 1, mtry = i)
MSE_rand[i] = mean((predict(rf_opt, test) - test$ozone)^2)
}
plot(MSE_rand)
for (i in 1:11){
temp_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = i)
MSE_rand[i] = mean((predict(temp_opt, test) - test$ozone)^2)
}
plot(MSE_rand)
min(MSE_rand)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 3)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=10000, nodesize = 1, mtry = 3)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=10000, nodesize = 1, mtry = 2)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=10000, nodesize = 1, mtry = 2)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
table((predict(rf_opt, test))
table((predict(rf_opt, test)))
table((predict(rf_opt, test)))
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 2, mtry = 2)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 3, mtry = 1)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 1)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 3)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 1)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 3)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
rf_opt = randomForest(ozone~., train,ntree=1000, nodesize = 1, mtry = 2)
rf_opt_mse = mean((predict(rf_opt, test) - test$ozone)^2)
help("randomForest")
install.packages(c('repr', 'IRkernel', 'IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')))
IRkernel::installspec()
install.packages("ggplot2")
library("agricolae", lib.loc="C:/Anaconda3/R/library")
library("AlgDesign", lib.loc="C:/Anaconda3/R/library")
detach("package:AlgDesign", unload=TRUE)
detach("package:agricolae", unload=TRUE)
library("ggplot2", lib.loc="C:/Anaconda3/R/library")
detach("package:ggplot2", unload=TRUE)
help(importance)
help("randomForest")
